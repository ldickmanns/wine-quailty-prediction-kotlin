{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066fcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%use kotlin-dl\n",
    "%use krangl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc50d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.jetbrains.kotlinx.dl.api.core.history.EpochTrainingEvent\n",
    "import org.jetbrains.kotlinx.dl.api.core.history.TrainingHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7724c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun normalize(df: DataFrame): DataFrame {\n",
    "    val normalizedCols = mutableListOf<DataCol>()\n",
    "    for (col in df.cols) {\n",
    "        val min = col.min()!!\n",
    "        val max = col.max()!!\n",
    "        val normalized = (col - min) / (max - min)\n",
    "        normalizedCols.add(normalized)\n",
    "    }\n",
    "    val normalizedDF = dataFrameOf(*normalizedCols.toTypedArray())\n",
    "    normalizedDF.setNames(*df.names.toTypedArray())\n",
    "    return normalizedDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca1b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun getXy(\n",
    "    df: DataFrame, \n",
    "    label: String = \"quality\"\n",
    "): Pair<Array<FloatArray>, FloatArray> {\n",
    "    val features = df.remove(label)\n",
    "    val nFeatures = features.ncol\n",
    "    val normalizedFeatures = normalize(features)\n",
    "    val columnsArray = normalizedFeatures.toFloatMatrix()\n",
    "    \n",
    "    val X = Array(features.nrow) { FloatArray(nFeatures) }\n",
    "\n",
    "    for (col in 0 until nFeatures) {\n",
    "        for (row in 0 until df.nrow) {\n",
    "            X[row][col] = columnsArray[col][row]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    val labels = df.get(label).toDoubles().filterNotNull().map { it.toFloat() }\n",
    "    \n",
    "    val y = labels.toFloatArray()\n",
    "    return Pair(X, y)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8afc82-d8c5-406b-a4a6-cb617ec2f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfTrain = DataFrame.readCSV(\"data/winequality-white-train.csv\")\n",
    "val dfValid = DataFrame.readCSV(\"data/winequality-white-val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "482f25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val (X, y) = getXy(dfTrain)\n",
    "val (XValid, yValid) = getXy(dfValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eeaec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31 0.25945947 0.21084337 0.012269938 0.12166172 0.020905923 0.3573086 0.102756895 0.47272727 0.22352941 0.45 5.0"
     ]
    }
   ],
   "source": [
    "X[0].forEach { print(\"$it \") }\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97d722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainDataset = OnHeapDataset.create(X, y)\n",
    "val validDataset = OnHeapDataset.create(XValid, yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf91cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintingCallback : Callback() {\n",
    "    override fun onEpochEnd(epoch: Int, event: EpochTrainingEvent, logs: TrainingHistory) {\n",
    "        println(\"Epoch: $epoch - loss: ${event.lossValue} - val loss: ${event.valLossValue}\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcd686d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = Sequential.of(\n",
    "    Input(11),\n",
    "    Dense(8),\n",
    "    Dense(8),\n",
    "    Dense(8),\n",
    "    Dense(8),\n",
    "    Dense(8),\n",
    "    Dense(8),\n",
    "    Dense(8),\n",
    "    Dense(1, activation = Activations.Linear)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "441da22f-9e44-4ba0-aa3b-0b1bda558917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adam(0.001f),\n",
    "    loss = Losses.MAE,\n",
    "    metric = Metrics.MAE,\n",
    "    callback = PrintingCallback(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9f56036-d332-445e-8a95-aa7aaeb89cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "Model type: Sequential\n",
      "______________________________________________________________________________\n",
      "Layer (type)                           Output Shape              Param #      \n",
      "==============================================================================\n",
      "input_1(Input)                         [None, 11]                0            \n",
      "______________________________________________________________________________\n",
      "dense_2(Dense)                         [None, 8]                 96           \n",
      "______________________________________________________________________________\n",
      "dense_3(Dense)                         [None, 8]                 72           \n",
      "______________________________________________________________________________\n",
      "dense_4(Dense)                         [None, 8]                 72           \n",
      "______________________________________________________________________________\n",
      "dense_5(Dense)                         [None, 8]                 72           \n",
      "______________________________________________________________________________\n",
      "dense_6(Dense)                         [None, 8]                 72           \n",
      "______________________________________________________________________________\n",
      "dense_7(Dense)                         [None, 8]                 72           \n",
      "______________________________________________________________________________\n",
      "dense_8(Dense)                         [None, 8]                 72           \n",
      "______________________________________________________________________________\n",
      "dense_9(Dense)                         [None, 1]                 9            \n",
      "______________________________________________________________________________\n",
      "==============================================================================\n",
      "Total trainable params: 537\n",
      "Total frozen params: 0\n",
      "Total params: 537\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "model.summary().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5c70e86-ac4b-4dc2-a7bf-b2a052fd6a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 5.172645568847656 - val loss: 0.8932986855506897\n",
      "Epoch: 2 - loss: 0.7548993229866028 - val loss: 0.7362748384475708\n",
      "Epoch: 3 - loss: 0.6922183632850647 - val loss: 0.6759459972381592\n",
      "Epoch: 4 - loss: 0.6693269610404968 - val loss: 0.6483440399169922\n",
      "Epoch: 5 - loss: 0.6548283696174622 - val loss: 0.6396056413650513\n",
      "Epoch: 6 - loss: 0.6440584063529968 - val loss: 0.6369010210037231\n",
      "Epoch: 7 - loss: 0.635884702205658 - val loss: 0.635438084602356\n",
      "Epoch: 8 - loss: 0.6286631226539612 - val loss: 0.6335959434509277\n",
      "Epoch: 9 - loss: 0.6233331561088562 - val loss: 0.6316449046134949\n",
      "Epoch: 10 - loss: 0.618718683719635 - val loss: 0.6287419199943542\n",
      "Epoch: 11 - loss: 0.6147053837776184 - val loss: 0.6243162155151367\n",
      "Epoch: 12 - loss: 0.6105544567108154 - val loss: 0.6200976371765137\n",
      "Epoch: 13 - loss: 0.6069340705871582 - val loss: 0.6162220239639282\n",
      "Epoch: 14 - loss: 0.6035922169685364 - val loss: 0.6125243306159973\n",
      "Epoch: 15 - loss: 0.6010518074035645 - val loss: 0.6085104942321777\n",
      "Epoch: 16 - loss: 0.5988627076148987 - val loss: 0.6078076958656311\n",
      "Epoch: 17 - loss: 0.5964861512184143 - val loss: 0.6067588925361633\n",
      "Epoch: 18 - loss: 0.5948807597160339 - val loss: 0.6052460670471191\n",
      "Epoch: 19 - loss: 0.5932630300521851 - val loss: 0.6035815477371216\n",
      "Epoch: 20 - loss: 0.5916348099708557 - val loss: 0.5992477536201477\n",
      "Epoch: 21 - loss: 0.5900794267654419 - val loss: 0.596018373966217\n",
      "Epoch: 22 - loss: 0.5889272689819336 - val loss: 0.5942642092704773\n",
      "Epoch: 23 - loss: 0.5876483917236328 - val loss: 0.5915138125419617\n",
      "Epoch: 24 - loss: 0.5867874026298523 - val loss: 0.5922355651855469\n",
      "Epoch: 25 - loss: 0.5858387351036072 - val loss: 0.589643120765686\n",
      "Epoch: 26 - loss: 0.5850035548210144 - val loss: 0.5910828113555908\n",
      "Epoch: 27 - loss: 0.5846569538116455 - val loss: 0.5901002287864685\n",
      "Epoch: 28 - loss: 0.5835177898406982 - val loss: 0.5905195474624634\n",
      "Epoch: 29 - loss: 0.5827451348304749 - val loss: 0.5880471467971802\n",
      "Epoch: 30 - loss: 0.5819796323776245 - val loss: 0.5879878401756287\n",
      "Epoch: 31 - loss: 0.581375777721405 - val loss: 0.5884472131729126\n",
      "Epoch: 32 - loss: 0.580832839012146 - val loss: 0.5884581208229065\n",
      "Epoch: 33 - loss: 0.5802301168441772 - val loss: 0.5887001156806946\n",
      "Epoch: 34 - loss: 0.5795914530754089 - val loss: 0.5881819725036621\n",
      "Epoch: 35 - loss: 0.5790215134620667 - val loss: 0.5885847806930542\n",
      "Epoch: 36 - loss: 0.5785090923309326 - val loss: 0.5879220366477966\n",
      "Epoch: 37 - loss: 0.5781506299972534 - val loss: 0.5865785479545593\n",
      "Epoch: 38 - loss: 0.5778578519821167 - val loss: 0.5861135721206665\n",
      "Epoch: 39 - loss: 0.5773330330848694 - val loss: 0.5868076086044312\n",
      "Epoch: 40 - loss: 0.5770268440246582 - val loss: 0.5868248343467712\n",
      "Epoch: 41 - loss: 0.5766605138778687 - val loss: 0.5865288972854614\n",
      "Epoch: 42 - loss: 0.5764155983924866 - val loss: 0.5872610807418823\n",
      "Epoch: 43 - loss: 0.5759029388427734 - val loss: 0.5860515236854553\n",
      "Epoch: 44 - loss: 0.5758569836616516 - val loss: 0.5872110724449158\n",
      "Epoch: 45 - loss: 0.5755533576011658 - val loss: 0.5863860845565796\n",
      "Epoch: 46 - loss: 0.5754650235176086 - val loss: 0.5861584544181824\n",
      "Epoch: 47 - loss: 0.5751826763153076 - val loss: 0.5871662497520447\n",
      "Epoch: 48 - loss: 0.5750145316123962 - val loss: 0.5858315229415894\n",
      "Epoch: 49 - loss: 0.5746996998786926 - val loss: 0.5852091312408447\n",
      "Epoch: 50 - loss: 0.5744786858558655 - val loss: 0.5851604342460632\n",
      "Epoch: 51 - loss: 0.5742620825767517 - val loss: 0.5843640565872192\n",
      "Epoch: 52 - loss: 0.5741349458694458 - val loss: 0.5850597023963928\n",
      "Epoch: 53 - loss: 0.5738988518714905 - val loss: 0.5845263600349426\n",
      "Epoch: 54 - loss: 0.5736602544784546 - val loss: 0.5859093070030212\n",
      "Epoch: 55 - loss: 0.5734113454818726 - val loss: 0.5853946805000305\n",
      "Epoch: 56 - loss: 0.5731801986694336 - val loss: 0.586581289768219\n",
      "Epoch: 57 - loss: 0.5730318427085876 - val loss: 0.5863194465637207\n",
      "Epoch: 58 - loss: 0.5729605555534363 - val loss: 0.5864193439483643\n",
      "Epoch: 59 - loss: 0.5727388858795166 - val loss: 0.5861449241638184\n",
      "Epoch: 60 - loss: 0.5724508762359619 - val loss: 0.5854256749153137\n",
      "Epoch: 61 - loss: 0.5721059441566467 - val loss: 0.5852922797203064\n",
      "Epoch: 62 - loss: 0.5719555616378784 - val loss: 0.5850463509559631\n",
      "Epoch: 63 - loss: 0.5717266201972961 - val loss: 0.5852246880531311\n",
      "Epoch: 64 - loss: 0.5716391801834106 - val loss: 0.5856026411056519\n",
      "Epoch: 65 - loss: 0.5714209079742432 - val loss: 0.585097074508667\n",
      "Epoch: 66 - loss: 0.5712319016456604 - val loss: 0.5844921469688416\n",
      "Epoch: 67 - loss: 0.5710727572441101 - val loss: 0.58526611328125\n",
      "Epoch: 68 - loss: 0.5708727836608887 - val loss: 0.5834057331085205\n",
      "Epoch: 69 - loss: 0.5708369016647339 - val loss: 0.5828766226768494\n",
      "Epoch: 70 - loss: 0.5706327557563782 - val loss: 0.582714319229126\n",
      "Epoch: 71 - loss: 0.570631206035614 - val loss: 0.5837007761001587\n",
      "Epoch: 72 - loss: 0.5704470872879028 - val loss: 0.5844656229019165\n",
      "Epoch: 73 - loss: 0.5704291462898254 - val loss: 0.584892749786377\n",
      "Epoch: 74 - loss: 0.5703487396240234 - val loss: 0.5842322707176208\n",
      "Epoch: 75 - loss: 0.5700712203979492 - val loss: 0.5849613547325134\n",
      "Epoch: 76 - loss: 0.5701687932014465 - val loss: 0.5863378643989563\n",
      "Epoch: 77 - loss: 0.5700161457061768 - val loss: 0.5854676961898804\n",
      "Epoch: 78 - loss: 0.5699018239974976 - val loss: 0.5840483903884888\n",
      "Epoch: 79 - loss: 0.5697080492973328 - val loss: 0.5842709541320801\n",
      "Epoch: 80 - loss: 0.5695563554763794 - val loss: 0.5849325656890869\n",
      "Epoch: 81 - loss: 0.5695432424545288 - val loss: 0.5855340957641602\n",
      "Epoch: 82 - loss: 0.5693276524543762 - val loss: 0.5845587253570557\n",
      "Epoch: 83 - loss: 0.5693868398666382 - val loss: 0.5845906734466553\n",
      "Epoch: 84 - loss: 0.5692912340164185 - val loss: 0.5847142338752747\n",
      "Epoch: 85 - loss: 0.5691852569580078 - val loss: 0.5845808386802673\n",
      "Epoch: 86 - loss: 0.5691173672676086 - val loss: 0.5842219591140747\n",
      "Epoch: 87 - loss: 0.5688109397888184 - val loss: 0.5851401090621948\n",
      "Epoch: 88 - loss: 0.5686416029930115 - val loss: 0.5841840505599976\n",
      "Epoch: 89 - loss: 0.5686477422714233 - val loss: 0.5844544768333435\n",
      "Epoch: 90 - loss: 0.5686352849006653 - val loss: 0.5855416655540466\n",
      "Epoch: 91 - loss: 0.5686267018318176 - val loss: 0.5845394134521484\n",
      "Epoch: 92 - loss: 0.5685563087463379 - val loss: 0.5861720442771912\n",
      "Epoch: 93 - loss: 0.5683842897415161 - val loss: 0.5856280326843262\n",
      "Epoch: 94 - loss: 0.568298876285553 - val loss: 0.5854190587997437\n",
      "Epoch: 95 - loss: 0.5682649612426758 - val loss: 0.585952877998352\n",
      "Epoch: 96 - loss: 0.5681493282318115 - val loss: 0.5850307941436768\n",
      "Epoch: 97 - loss: 0.5680619478225708 - val loss: 0.5856742858886719\n",
      "Epoch: 98 - loss: 0.5680328011512756 - val loss: 0.5856652855873108\n",
      "Epoch: 99 - loss: 0.5680355429649353 - val loss: 0.5858222246170044\n",
      "Epoch: 100 - loss: 0.5678897500038147 - val loss: 0.5857934355735779\n",
      "Epoch: 101 - loss: 0.5678344368934631 - val loss: 0.5846637487411499\n",
      "Epoch: 102 - loss: 0.5679241418838501 - val loss: 0.5858043432235718\n",
      "Epoch: 103 - loss: 0.5678805112838745 - val loss: 0.5864064693450928\n",
      "Epoch: 104 - loss: 0.567782461643219 - val loss: 0.5861815214157104\n",
      "Epoch: 105 - loss: 0.5676923990249634 - val loss: 0.5859919190406799\n",
      "Epoch: 106 - loss: 0.5676553845405579 - val loss: 0.5850258469581604\n",
      "Epoch: 107 - loss: 0.5675951838493347 - val loss: 0.5857459902763367\n",
      "Epoch: 108 - loss: 0.5674763321876526 - val loss: 0.58596271276474\n",
      "Epoch: 109 - loss: 0.5673322677612305 - val loss: 0.5859946012496948\n",
      "Epoch: 110 - loss: 0.5673098564147949 - val loss: 0.5862399935722351\n",
      "Epoch: 111 - loss: 0.5672772526741028 - val loss: 0.5851472616195679\n",
      "Epoch: 112 - loss: 0.5672799944877625 - val loss: 0.5865275263786316\n",
      "Epoch: 113 - loss: 0.5672729015350342 - val loss: 0.5867859125137329\n",
      "Epoch: 114 - loss: 0.5672509670257568 - val loss: 0.5861791968345642\n",
      "Epoch: 115 - loss: 0.5673073530197144 - val loss: 0.588655412197113\n",
      "Epoch: 116 - loss: 0.56735759973526 - val loss: 0.587699294090271\n",
      "Epoch: 117 - loss: 0.5670719146728516 - val loss: 0.5879983901977539\n",
      "Epoch: 118 - loss: 0.5671196579933167 - val loss: 0.5885245203971863\n",
      "Epoch: 119 - loss: 0.5670013427734375 - val loss: 0.5875733494758606\n",
      "Epoch: 120 - loss: 0.5670244097709656 - val loss: 0.5879049897193909\n",
      "Epoch: 121 - loss: 0.5670146346092224 - val loss: 0.5893456935882568\n",
      "Epoch: 122 - loss: 0.5669770240783691 - val loss: 0.5886294841766357\n",
      "Epoch: 123 - loss: 0.5669223070144653 - val loss: 0.5882338285446167\n",
      "Epoch: 124 - loss: 0.5670063495635986 - val loss: 0.5894520878791809\n",
      "Epoch: 125 - loss: 0.5669310092926025 - val loss: 0.5890834331512451\n",
      "Epoch: 126 - loss: 0.5667233467102051 - val loss: 0.5894216895103455\n",
      "Epoch: 127 - loss: 0.566667377948761 - val loss: 0.5890562534332275\n",
      "Epoch: 128 - loss: 0.56663978099823 - val loss: 0.5890289545059204\n",
      "Epoch: 129 - loss: 0.5665398836135864 - val loss: 0.5888396501541138\n",
      "Epoch: 130 - loss: 0.5665097832679749 - val loss: 0.5882090330123901\n",
      "Epoch: 131 - loss: 0.5667601823806763 - val loss: 0.5901376008987427\n",
      "Epoch: 132 - loss: 0.5663723349571228 - val loss: 0.5887855887413025\n",
      "Epoch: 133 - loss: 0.5664577484130859 - val loss: 0.5898470878601074\n",
      "Epoch: 134 - loss: 0.5662180185317993 - val loss: 0.5892878770828247\n",
      "Epoch: 135 - loss: 0.5662600994110107 - val loss: 0.5906988382339478\n",
      "Epoch: 136 - loss: 0.5662496089935303 - val loss: 0.5888619422912598\n",
      "Epoch: 137 - loss: 0.5663594007492065 - val loss: 0.589483380317688\n",
      "Epoch: 138 - loss: 0.5664074420928955 - val loss: 0.5893918871879578\n",
      "Epoch: 139 - loss: 0.5662798881530762 - val loss: 0.5898559093475342\n",
      "Epoch: 140 - loss: 0.5661695003509521 - val loss: 0.5904378890991211\n",
      "Epoch: 141 - loss: 0.5662648677825928 - val loss: 0.5898751020431519\n",
      "Epoch: 142 - loss: 0.5661699771881104 - val loss: 0.5906526446342468\n",
      "Epoch: 143 - loss: 0.5660477876663208 - val loss: 0.5894128084182739\n",
      "Epoch: 144 - loss: 0.566189706325531 - val loss: 0.591038167476654\n",
      "Epoch: 145 - loss: 0.566281795501709 - val loss: 0.5906999111175537\n",
      "Epoch: 146 - loss: 0.5660078525543213 - val loss: 0.5906299352645874\n",
      "Epoch: 147 - loss: 0.5659365057945251 - val loss: 0.5909163355827332\n",
      "Epoch: 148 - loss: 0.5658105611801147 - val loss: 0.5911993384361267\n",
      "Epoch: 149 - loss: 0.5658246874809265 - val loss: 0.5914961099624634\n",
      "Epoch: 150 - loss: 0.5660663843154907 - val loss: 0.5917826890945435\n",
      "Epoch: 151 - loss: 0.5658573508262634 - val loss: 0.5922563076019287\n",
      "Epoch: 152 - loss: 0.5658873915672302 - val loss: 0.591772735118866\n",
      "Epoch: 153 - loss: 0.5656753778457642 - val loss: 0.5920410752296448\n",
      "Epoch: 154 - loss: 0.565713107585907 - val loss: 0.5915587544441223\n",
      "Epoch: 155 - loss: 0.5655622482299805 - val loss: 0.5913450717926025\n",
      "Epoch: 156 - loss: 0.565398097038269 - val loss: 0.5920379757881165\n",
      "Epoch: 157 - loss: 0.5657430291175842 - val loss: 0.5927905440330505\n",
      "Epoch: 158 - loss: 0.5655751824378967 - val loss: 0.592510461807251\n",
      "Epoch: 159 - loss: 0.565506100654602 - val loss: 0.5919662714004517\n",
      "Epoch: 160 - loss: 0.5653442740440369 - val loss: 0.592095673084259\n",
      "Epoch: 161 - loss: 0.5656184554100037 - val loss: 0.5919880270957947\n",
      "Epoch: 162 - loss: 0.5651568174362183 - val loss: 0.5927671194076538\n",
      "Epoch: 163 - loss: 0.565485417842865 - val loss: 0.5921783447265625\n",
      "Epoch: 164 - loss: 0.5652552843093872 - val loss: 0.5926535725593567\n",
      "Epoch: 165 - loss: 0.5652742385864258 - val loss: 0.5925433039665222\n",
      "Epoch: 166 - loss: 0.5650250315666199 - val loss: 0.5920185446739197\n",
      "Epoch: 167 - loss: 0.5650287866592407 - val loss: 0.5919009447097778\n",
      "Epoch: 168 - loss: 0.5649824142456055 - val loss: 0.5927379727363586\n",
      "Epoch: 169 - loss: 0.565060555934906 - val loss: 0.5925648808479309\n",
      "Epoch: 170 - loss: 0.5648856163024902 - val loss: 0.592464029788971\n",
      "Epoch: 171 - loss: 0.5650439858436584 - val loss: 0.5930803418159485\n",
      "Epoch: 172 - loss: 0.564971387386322 - val loss: 0.5929689407348633\n",
      "Epoch: 173 - loss: 0.5647985339164734 - val loss: 0.5925090909004211\n",
      "Epoch: 174 - loss: 0.5646211504936218 - val loss: 0.5931404829025269\n",
      "Epoch: 175 - loss: 0.5646620392799377 - val loss: 0.593303918838501\n",
      "Epoch: 176 - loss: 0.5647404193878174 - val loss: 0.5931867957115173\n",
      "Epoch: 177 - loss: 0.564502477645874 - val loss: 0.5938377976417542\n",
      "Epoch: 178 - loss: 0.5644809603691101 - val loss: 0.5932915806770325\n",
      "Epoch: 179 - loss: 0.5644907355308533 - val loss: 0.5939750671386719\n",
      "Epoch: 180 - loss: 0.5643230676651001 - val loss: 0.5935931205749512\n",
      "Epoch: 181 - loss: 0.5643555521965027 - val loss: 0.594088077545166\n",
      "Epoch: 182 - loss: 0.5643283724784851 - val loss: 0.5940068960189819\n",
      "Epoch: 183 - loss: 0.5640621781349182 - val loss: 0.5945127606391907\n",
      "Epoch: 184 - loss: 0.564034640789032 - val loss: 0.5940753817558289\n",
      "Epoch: 185 - loss: 0.5641932487487793 - val loss: 0.5940244793891907\n",
      "Epoch: 186 - loss: 0.5639874935150146 - val loss: 0.5937291383743286\n",
      "Epoch: 187 - loss: 0.5638265609741211 - val loss: 0.5938973426818848\n",
      "Epoch: 188 - loss: 0.5637379884719849 - val loss: 0.5942394733428955\n",
      "Epoch: 189 - loss: 0.5638301372528076 - val loss: 0.5942865014076233\n",
      "Epoch: 190 - loss: 0.5636696815490723 - val loss: 0.5950928926467896\n",
      "Epoch: 191 - loss: 0.5637145638465881 - val loss: 0.5948742032051086\n",
      "Epoch: 192 - loss: 0.563641369342804 - val loss: 0.5949016213417053\n",
      "Epoch: 193 - loss: 0.5634763240814209 - val loss: 0.594565749168396\n",
      "Epoch: 194 - loss: 0.563591480255127 - val loss: 0.5947269797325134\n",
      "Epoch: 195 - loss: 0.5633211731910706 - val loss: 0.5942691564559937\n",
      "Epoch: 196 - loss: 0.5632185935974121 - val loss: 0.5955883860588074\n",
      "Epoch: 197 - loss: 0.5636323094367981 - val loss: 0.5953059792518616\n",
      "Epoch: 198 - loss: 0.5631154179573059 - val loss: 0.5944944024085999\n",
      "Epoch: 199 - loss: 0.5629122853279114 - val loss: 0.5946999192237854\n",
      "Epoch: 200 - loss: 0.5630835294723511 - val loss: 0.5948855876922607\n",
      "Epoch: 201 - loss: 0.563087522983551 - val loss: 0.5950474739074707\n",
      "Epoch: 202 - loss: 0.5630244612693787 - val loss: 0.5943456888198853\n",
      "Epoch: 203 - loss: 0.5626876354217529 - val loss: 0.5950718522071838\n",
      "Epoch: 204 - loss: 0.5625514984130859 - val loss: 0.5952059030532837\n",
      "Epoch: 205 - loss: 0.5625672340393066 - val loss: 0.5955443382263184\n",
      "Epoch: 206 - loss: 0.5625756978988647 - val loss: 0.5959609150886536\n",
      "Epoch: 207 - loss: 0.5624791383743286 - val loss: 0.5957806706428528\n",
      "Epoch: 208 - loss: 0.562406599521637 - val loss: 0.5962729454040527\n",
      "Epoch: 209 - loss: 0.562384307384491 - val loss: 0.5965590476989746\n",
      "Epoch: 210 - loss: 0.5625108480453491 - val loss: 0.5964557528495789\n",
      "Epoch: 211 - loss: 0.5625245571136475 - val loss: 0.5964828133583069\n",
      "Epoch: 212 - loss: 0.5625372529029846 - val loss: 0.5965383648872375\n",
      "Epoch: 213 - loss: 0.5623476505279541 - val loss: 0.5969740152359009\n",
      "Epoch: 214 - loss: 0.5621643662452698 - val loss: 0.5965718626976013\n",
      "Epoch: 215 - loss: 0.5623389482498169 - val loss: 0.5974688529968262\n",
      "Epoch: 216 - loss: 0.5622656345367432 - val loss: 0.5981824398040771\n",
      "Epoch: 217 - loss: 0.5621986985206604 - val loss: 0.5973914265632629\n",
      "Epoch: 218 - loss: 0.5622744560241699 - val loss: 0.5975044965744019\n",
      "Epoch: 219 - loss: 0.5620756149291992 - val loss: 0.5982291102409363\n",
      "Epoch: 220 - loss: 0.5622656941413879 - val loss: 0.5985953211784363\n",
      "Epoch: 221 - loss: 0.5620620250701904 - val loss: 0.5987587571144104\n",
      "Epoch: 222 - loss: 0.561654806137085 - val loss: 0.5977786183357239\n",
      "Epoch: 223 - loss: 0.5616896748542786 - val loss: 0.5982739329338074\n",
      "Epoch: 224 - loss: 0.5616980791091919 - val loss: 0.5988004207611084\n",
      "Epoch: 225 - loss: 0.5617582201957703 - val loss: 0.5984446406364441\n",
      "Epoch: 226 - loss: 0.5615466833114624 - val loss: 0.5986907482147217\n",
      "Epoch: 227 - loss: 0.5614475011825562 - val loss: 0.5984349250793457\n",
      "Epoch: 228 - loss: 0.5613972544670105 - val loss: 0.5982157588005066\n",
      "Epoch: 229 - loss: 0.561282217502594 - val loss: 0.5986533164978027\n",
      "Epoch: 230 - loss: 0.5611233115196228 - val loss: 0.5980687737464905\n",
      "Epoch: 231 - loss: 0.5611395835876465 - val loss: 0.5982599854469299\n",
      "Epoch: 232 - loss: 0.561215341091156 - val loss: 0.5987498760223389\n",
      "Epoch: 233 - loss: 0.5611725449562073 - val loss: 0.5989850759506226\n",
      "Epoch: 234 - loss: 0.5610230565071106 - val loss: 0.5992600321769714\n",
      "Epoch: 235 - loss: 0.5610975027084351 - val loss: 0.5990449786186218\n",
      "Epoch: 236 - loss: 0.5611203908920288 - val loss: 0.5991337895393372\n",
      "Epoch: 237 - loss: 0.5609055161476135 - val loss: 0.5995019674301147\n",
      "Epoch: 238 - loss: 0.5609241127967834 - val loss: 0.599625289440155\n",
      "Epoch: 239 - loss: 0.5608456134796143 - val loss: 0.5995774865150452\n",
      "Epoch: 240 - loss: 0.5608232617378235 - val loss: 0.5995025634765625\n",
      "Epoch: 241 - loss: 0.5606271028518677 - val loss: 0.5999362468719482\n",
      "Epoch: 242 - loss: 0.560687243938446 - val loss: 0.5993967652320862\n",
      "Epoch: 243 - loss: 0.5604785680770874 - val loss: 0.5996628403663635\n",
      "Epoch: 244 - loss: 0.5605745911598206 - val loss: 0.5995986461639404\n",
      "Epoch: 245 - loss: 0.5604435801506042 - val loss: 0.5984985828399658\n",
      "Epoch: 246 - loss: 0.560512125492096 - val loss: 0.6001989245414734\n",
      "Epoch: 247 - loss: 0.5603532791137695 - val loss: 0.5993899703025818\n",
      "Epoch: 248 - loss: 0.5602385401725769 - val loss: 0.6000337600708008\n",
      "Epoch: 249 - loss: 0.5602006316184998 - val loss: 0.5995879173278809\n",
      "Epoch: 250 - loss: 0.5599183440208435 - val loss: 0.5986396074295044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "org.jetbrains.kotlinx.dl.api.core.history.TrainingHistory@54e22bdd"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    trainingDataset = trainDataset,\n",
    "    validationDataset = validDataset,\n",
    "    epochs = 250,\n",
    "    trainBatchSize = 32,\n",
    "    validationBatchSize = 1024\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58a0aaa5-8aaa-4a37-a5c7-21863245c81d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5996628403663635\n"
     ]
    }
   ],
   "source": [
    "val result = model.evaluate(validDataset)\n",
    "println(result.lossValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc6fd598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><body><table><tr><th style=\"text-align:left\">fixed acidity</th><th style=\"text-align:left\">volatile acidity</th><th style=\"text-align:left\">citric acid</th><th style=\"text-align:left\">residual sugar</th><th style=\"text-align:left\">chlorides</th><th style=\"text-align:left\">free sulfur dioxide</th><th style=\"text-align:left\">total sulfur dioxide</th><th style=\"text-align:left\">density</th><th style=\"text-align:left\">pH</th><th style=\"text-align:left\">sulphates</th><th style=\"text-align:left\">alcohol</th><th style=\"text-align:left\">quality</th></tr><tr><td style=\"text-align:left\" title=\"7.0\">7.0</td><td style=\"text-align:left\" title=\"0.27\">0.27</td><td style=\"text-align:left\" title=\"0.36\">0.36</td><td style=\"text-align:left\" title=\"20.7\">20.7</td><td style=\"text-align:left\" title=\"0.045\">0.045</td><td style=\"text-align:left\" title=\"45.0\">45.0</td><td style=\"text-align:left\" title=\"170.0\">170.0</td><td style=\"text-align:left\" title=\"1.001\">1.001</td><td style=\"text-align:left\" title=\"3.0\">3.0</td><td style=\"text-align:left\" title=\"0.45\">0.45</td><td style=\"text-align:left\" title=\"8.8\">8.8</td><td style=\"text-align:left\" title=\"6\">6</td></tr><tr><td style=\"text-align:left\" title=\"6.4\">6.4</td><td style=\"text-align:left\" title=\"0.31\">0.31</td><td style=\"text-align:left\" title=\"0.38\">0.38</td><td style=\"text-align:left\" title=\"2.9\">2.9</td><td style=\"text-align:left\" title=\"0.038\">0.038</td><td style=\"text-align:left\" title=\"19.0\">19.0</td><td style=\"text-align:left\" title=\"102.0\">102.0</td><td style=\"text-align:left\" title=\"0.9912\">0.9912</td><td style=\"text-align:left\" title=\"3.17\">3.17</td><td style=\"text-align:left\" title=\"0.35\">0.35</td><td style=\"text-align:left\" title=\"11.0\">11.0</td><td style=\"text-align:left\" title=\"7\">7</td></tr><tr><td style=\"text-align:left\" title=\"6.6\">6.6</td><td style=\"text-align:left\" title=\"0.27\">0.27</td><td style=\"text-align:left\" title=\"0.41\">0.41</td><td style=\"text-align:left\" title=\"1.3\">1.3</td><td style=\"text-align:left\" title=\"0.052\">0.052</td><td style=\"text-align:left\" title=\"16.0\">16.0</td><td style=\"text-align:left\" title=\"142.0\">142.0</td><td style=\"text-align:left\" title=\"0.9951\">0.9951</td><td style=\"text-align:left\" title=\"3.42\">3.42</td><td style=\"text-align:left\" title=\"0.47\">0.47</td><td style=\"text-align:left\" title=\"10.0\">10.0</td><td style=\"text-align:left\" title=\"6\">6</td></tr><tr><td style=\"text-align:left\" title=\"7.0\">7.0</td><td style=\"text-align:left\" title=\"0.25\">0.25</td><td style=\"text-align:left\" title=\"0.32\">0.32</td><td style=\"text-align:left\" title=\"9.0\">9.0</td><td style=\"text-align:left\" title=\"0.046\">0.046</td><td style=\"text-align:left\" title=\"56.0\">56.0</td><td style=\"text-align:left\" title=\"245.0\">245.0</td><td style=\"text-align:left\" title=\"0.9955\">0.9955</td><td style=\"text-align:left\" title=\"3.25\">3.25</td><td style=\"text-align:left\" title=\"0.5\">0.5</td><td style=\"text-align:left\" title=\"10.4\">10.4</td><td style=\"text-align:left\" title=\"6\">6</td></tr><tr><td style=\"text-align:left\" title=\"6.6\">6.6</td><td style=\"text-align:left\" title=\"0.38\">0.38</td><td style=\"text-align:left\" title=\"0.15\">0.15</td><td style=\"text-align:left\" title=\"4.6\">4.6</td><td style=\"text-align:left\" title=\"0.044\">0.044</td><td style=\"text-align:left\" title=\"25.0\">25.0</td><td style=\"text-align:left\" title=\"78.0\">78.0</td><td style=\"text-align:left\" title=\"0.9931\">0.9931</td><td style=\"text-align:left\" title=\"3.11\">3.11</td><td style=\"text-align:left\" title=\"0.38\">0.38</td><td style=\"text-align:left\" title=\"10.2\">10.2</td><td style=\"text-align:left\" title=\"6\">6</td></tr><tr><td style=\"text-align:left\" title=\"6.9\">6.9</td><td style=\"text-align:left\" title=\"0.21\">0.21</td><td style=\"text-align:left\" title=\"0.33\">0.33</td><td style=\"text-align:left\" title=\"1.8\">1.8</td><td style=\"text-align:left\" title=\"0.034\">0.034</td><td style=\"text-align:left\" title=\"48.0\">48.0</td><td style=\"text-align:left\" title=\"136.0\">136.0</td><td style=\"text-align:left\" title=\"0.9899\">0.9899</td><td style=\"text-align:left\" title=\"3.25\">3.25</td><td style=\"text-align:left\" title=\"0.41\">0.41</td><td style=\"text-align:left\" title=\"12.6\">12.6</td><td style=\"text-align:left\" title=\"7\">7</td></tr></table><p>... with 484 more rows. Shape: 490 x 12. \n",
       "</p></body></html>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTest = DataFrame.readCSV(\"data/winequality-white-test.csv\")\n",
    "dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e5b1183-cde6-4a44-9348-55a6deb7bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "val (XTest, yTest) = getXy(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01e2c8d-0cb0-4981-94bd-b52c151df814",
   "metadata": {},
   "outputs": [],
   "source": [
    "val testDataset = OnHeapDataset.create(XTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031f107f-b66d-4af9-94d8-38c35ee97b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(lossValue=0.8474463820457458, metrics={MAE=0.8474463820457458})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13a3ed-8b43-4ec3-9011-878d93ecc018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.7.0-dev-3303"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
